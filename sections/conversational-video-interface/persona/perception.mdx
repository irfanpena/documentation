---
title: Perception
sidebarTitle: Perception
description: Learn how to configure the perception layer with Raven to enable the real-time visual understanding.
---

The **Perception Layer** in Tavus enhances an AI agent with real-time visual understanding.
By using <a href="/sections/models#raven%3A-perception-model" target="_blank" rel="noopener noreferrer"><strong>Raven</strong></a>, the AI agent becomes more context-aware, responsive, and capable of triggering actions based on visual input.

## Configuring the Perception Layer
To configure the Perception Layer, define the following parameters within the `layers.perception` object:
### 1. `perception_model`
Specifies the perception model to use.
- **Options**:
  - `raven-0` (default and recommended): Advanced visual capabilities, including screen share support, ambient queries, and perception tools.
  - `basic`: Legacy model with limited features.
  - `off`: Disables the perception layer.
<Note>
**Screen Share Feature**:When using `raven-0`, screen share feature is enabled by default without additional configuration.
</Note>

```json
"layers": {
  "perception": {
    "perception_model": "raven-0"
  }
}
```

### 2. `ambient_awareness_queries`
An array of custom queries that `raven-0` continuously monitors in the visual stream.

```json
"ambient_awareness_queries": [
  "Is the user wearing a bright outfit?"
]
```

### 3. `perception_analysis_queries`
An array of custom queries that `raven-0` processes at the end of the call to generate a visual analysis summary for the user.
```json
"perception_analysis_queries": [
  "Is the user wearing multiple bright colors?",
  "Is there any indication that more than one person is present?",
  "On a scale of 1-100, how often was the user looking at the screen?"
]
```

<Tip>
Best practices for `ambient_awareness_queries` and `perception_analysis_queries`:
- Use simple, focused prompts.
- Use queries that support your personaâ€™s purpose.
</Tip>

### 4. `perception_tool_prompt`

Tell `raven-0` when and how to trigger tools based on what it sees.
```json
"perception_tool_prompt":
  "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected."
```

### 5. `perception_tools`
Defines callable functions that `raven-0` can trigger upon detecting specific visual conditions. Each tool must include a `type` and a `function` object detailing its schema.

```json
"perception_tools": [
  {
    "type": "function",
    "function": {
      "name": "notify_if_bright_outfit_shown",
      "description": "Use this function when a bright outfit is detected in the image with high confidence",
      "parameters": {
        "type": "object",
        "properties": {
          "outfit_color": {
            "type": "string",
            "description": "Best guess on what color of outfit it is"
          }
        },
        "required": ["outfit_color"]
      }
    }
  }
]
```

<Note>
Please see <a href="/sections/conversational-video-interface/persona/perception-tool" target="_blank" rel="noopener noreferrer">Tool Calling</a> for more details.
</Note>

## Example Configuration
This example demonstrates a persona designed to identify when a user wears a bright outfit and triggers an internal action accordingly.

```json
{
  "persona_name": "Fashion Advisor",
  "system_prompt": "As a Fashion Advisor, you specialize in offering tailored fashion advice.",
  "pipeline_mode": "full",
  "context": "You're having a video conversation with a client about their outfit.",
  "default_replica_id": "r79e1c033f",
  "layers": {
    "perception": {
      "perception_model": "raven-0",
      "ambient_awareness_queries": [
        "Is the user wearing a bright outfit?"
      ],
      "perception_analysis_queries": [
        "Is the user wearing multiple bright colors?",
        "Is there any indication that more than one person is present?",
        "On a scale of 1-100, how often was the user looking at the screen?"
      ],
      "perception_tool_prompt": "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected.",
      "perception_tools": [
        {
          "type": "function",
          "function": {
            "name": "notify_if_bright_outfit_shown",
            "description": "Use this function when a bright outfit is detected in the image with high confidence",
            "parameters": {
              "type": "object",
              "properties": {
                "outfit_color": {
                  "type": "string",
                  "description": "Best guess on what color of outfit it is"
                }
              },
              "required": ["outfit_color"]
            }
          }
        }
      ]
    }
  }
}
```
<Note>
Please see the <a href="/api-reference/personas/create-persona" target='_blank'>Create a Persona</a> endpoint for more details.
</Note>

## End-of-call Perception Analysis

`raven-0` generates a visual summary at the end of a call. This summary includes all detected visual artifacts and will be sent as <a href="/sections/event-schemas/conversation-perception-analysis" target="_blank">Perception Analysis</a> event to the <a href="/sections/webhooks-and-callbacks#conversation-callbacks" target="_blank">conversation callback</a> (if specified)

<Note>
This feature is exclusive to personas with `raven-0` specified in the Perception Layer.
</Note>

Your backend will receive the visual summary once it's processed. For example, using the [Example Persona](#example-configuration) will produce this visual summary.

```json
{
  "properties": {
    "analysis": "Here's a summary of the visual observations:\n\n*   **User Appearance:** The subject is a young person, likely in their teens or early twenties, with dark hair and an East Asian appearance. They consistently wear a dark blue or black hooded jacket/hoodie with pink and white accents, patterns, or text on the sleeves, and possibly a white undershirt. A pendant or charm was observed on their chest. The setting is consistently an indoor environment with a plain white or light-colored wall background.\n*   **User Behavior and Demeanor:** The user frequently holds a wired earpiece, microphone, or earbuds near their mouth or chin, appearing to be speaking, listening intently, or in deep thought. Their gaze is predominantly cast downwards, occasionally looking slightly off to the side, with only rare, brief glances forward. They generally maintain a still posture.\n*   **User Emotions:** The user's expression is consistently neutral, conveying a sense of quiet concentration, engagement, contemplation, or thoughtful introspection. There are no overt signs of strong emotion; their demeanor is described as calm, focused, sometimes pensive, or slightly subdued. They appear to be actively listening or processing information.\n*   **Screen Gaze:** Based on the observations that the user's eyes were generally cast downwards, looking off to the side, or only occasionally glancing forward, the user was looking at the screen approximately **5-10%** of the time."
  },
  "conversation_id": "<conversation_id>",
  "webhook_url": "<webhook_url>",
  "message_type": "application",
  "event_type": "application.perception_analysis",
  "timestamp": "2025-07-11T09:13:35.361736Z"
}

```

Based on the visual summary, the model utilizes the `ambient_awareness_queries` property to continuously analyze the user's outfit, identifying details such as the type of clothing, color, and accessories. Additionally, through the `perception_analysis_queries` property, the model can estimate how long the user is looking at the screen.
