---
title: Perception
sidebarTitle: Perception
description: Learn how to configure the perception layer with Raven to enable the real-time visual understanding.
---

The **Perception Layer** in Tavus enhances an AI agent with real-time visual understanding.
By using <a href="/sections/models#raven%3A-perception-model" target="_blank" rel="noopener noreferrer"><strong>Raven</strong></a>, the AI agent becomes more context-aware, responsive, and capable of triggering actions based on visual input.

## Configuring the Perception Layer
To configure the Perception Layer, define the following parameters within the `layers.perception` object:
### 1. `perception_model`
Specifies the perception model to use.
- **Options**:
  - `raven-0` (default and recommended): Advanced visual capabilities, including screen share support, ambient queries, and perception tools.
  - `basic`: Legacy model with limited features.
  - `off`: Disables the perception layer.
<Note>
**Screen Share Feature**:When using `raven-0`, screen share feature is enabled by default without additional configuration.
</Note>

```json
"layers": {
  "perception": {
    "perception_model": "raven-0"
  }
}
```

### 2. `ambient_awareness_queries`
An array of custom queries that `raven-0` continuously monitors in the visual stream.

```json
"ambient_awareness_queries": [
  "Is the user wearing a bright outfit?"
]
```

### 3. `perception_analysis_queries`
An array of custom queries that `raven-0` processes at the end of the call to generate a visual analysis summary for the user.
```json
"perception_analysis_queries": [
  "Is the user wearing multiple bright colors?",
  "Is there any indication that more than one person is present?"
]
```

<Tip>
Best practices for `ambient_awareness_queries` and `perception_analysis_queries`:
- Use simple, focused prompts.
- Use queries that support your personaâ€™s purpose.
</Tip>

### 4. `perception_tool_prompt`

Tell `raven-0` when and how to trigger tools based on what it sees.
```json
"perception_tool_prompt":
  "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected."
```

### 5. `perception_tools`
Defines callable functions that `raven-0` can trigger upon detecting specific visual conditions. Each tool must include a `type` and a `function` object detailing its schema.

```json
"perception_tools": [
  {
    "type": "function",
    "function": {
      "name": "notify_if_bright_outfit_shown",
      "description": "Use this function when a bright outfit is detected in the image with high confidence",
      "parameters": {
        "type": "object",
        "properties": {
          "outfit_color": {
            "type": "string",
            "description": "Best guess on what color of outfit it is"
          }
        },
        "required": ["outfit_color"]
      }
    }
  }
]
```

<Note>
Please see <a href="/sections/conversational-video-interface/persona/perception-tool" target="_blank" rel="noopener noreferrer">Tool Calling</a> for more details.
</Note>

## Example Configuration
This example demonstrates a persona designed to identify when a user wears a bright outfit and triggers an internal action accordingly.

```json
{
  "persona_name": "Fashion Advisor",
  "system_prompt": "As a Fashion Advisor, you specialize in offering tailored fashion advice.",
  "pipeline_mode": "full",
  "context": "You're having a video conversation with a client about their outfit.",
  "default_replica_id": "r79e1c033f",
  "layers": {
    "perception": {
      "perception_model": "raven-0",
      "ambient_awareness_queries": [
        "How many person is present?",
        "Is the user looking at the screen?"
      ],
      "perception_analysis_queries": [
        "Is there any indication that more than one person is present?",
        "On a scale of 1-100, how often was the user looking at the screen?"
      ],
      "perception_tool_prompt": "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected.",
      "perception_tools": [
        {
          "type": "function",
          "function": {
            "name": "notify_if_bright_outfit_shown",
            "description": "Use this function when a bright outfit is detected in the image with high confidence",
            "parameters": {
              "type": "object",
              "properties": {
                "outfit_color": {
                  "type": "string",
                  "description": "Best guess on what color of outfit it is"
                }
              },
              "required": ["outfit_color"]
            }
          }
        }
      ]
    }
  }
}
```
<Note>
Please see the <a href="/api-reference/personas/create-persona" target='_blank'>Create a Persona</a> endpoint for more details.
</Note>

## End-of-call Perception Analysis

`raven-0` generates a visual summary at the end of a call. This summary includes all detected visual artifacts and will be sent as <a href="https://docs.tavus.io/sections/event-schemas/conversation-perception-analysis" target="_blank">Perception Analysis</a> event to the <a href="https://docs.tavus.io/sections/conversational-video-interface/conversation-callbacks" target="_blank">conversation callback</a> (if specified)

<Note>
This feature is exclusive to personas with `raven-0` specified in the Perception Layer.
</Note>

Your backend will receive the visual summary once it's processed. For [Example Persona](#example-configuration) will produce this visual summary

<CodeGroup>

```json with ambient_awareness_queries
{
  "properties": {
    "analysis": "Here's a summary of the visual observations:\n\n*   **Presence of multiple people:** There is no indication that more than one person was present during the video call. All observations consistently state that only one person was visible in the frame.\n*   **User's gaze towards the screen:** On a scale of 1-100, the user was looking at the screen approximately 75% of the time. While there was one instance where their gaze was averted, for the majority of the observations, the user was either looking directly at the screen or in its general direction."
  },
  "conversation_id": "<your_conversation_id>",
  "webhook_url": "<your_webhook_url>",
  "message_type": "application",
  "event_type": "application.perception_analysis",
  "timestamp": "2025-07-11T06:57:35.036251Z"
}
```

```json without ambient_awareness_queries
{
  "properties": {
    "analysis": "Here's a summary of the visual observations:\n\n*   **Presence of Others:** \nThere is no indication that more than one person is present in the video call. Several observations explicitly state that no other people or pets were visible in the background.\n*   **User Gaze Towards Screen:** The user was looking at the screen approximately **75%** of the time. While they consistently looked directly at the camera for the majority of the observed period, there were instances where their gaze shifted upwards, to the side, or was not fixed on the camera."
  },
  "conversation_id": "<your_conversation_id>",
  "webhook_url": "<your_webhook_url>",
  "event_type": "application.perception_analysis",
  "message_type": "application",
  "timestamp": "2025-07-11T07:06:24.209090Z"
}
```
  
</CodeGroup>

`ambient_awareness_queries` is not making noticeable difference in analysis